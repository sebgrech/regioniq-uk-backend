name: RegionIQ Weekly Forecast Pipeline

on:
  schedule:
    # Run every Sunday at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:  # Allow manual trigger from GitHub UI

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # Fail if pipeline takes > 2 hours
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install duckdb pandas numpy scipy statsmodels requests psycopg2-binary python-dotenv
      
      - name: Configure environment
        run: |
          # Create required directories
          mkdir -p data/logs data/lake data/raw data/silver data/export
          # Verify geography lookup exists
          test -f data/reference/master_2025_geography_lookup.csv || exit 1
      
      - name: Set up Supabase credentials
        env:
          SUPABASE_URI: ${{ secrets.SUPABASE_URI }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        run: |
          cat > .env << EOF
          SUPABASE_URI=$SUPABASE_URI
          SUPABASE_URL=$SUPABASE_URL
          SUPABASE_ANON_KEY=$SUPABASE_ANON_KEY
          EOF
      
      - name: Run forecast pipeline
        id: pipeline
        run: python3 run_full_forecast_pipeline.py
        continue-on-error: true  # Continue to upload logs even if pipeline fails
      
      - name: Upload pipeline logs
        if: always()  # Upload logs even on failure
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs-${{ github.run_id }}
          path: data/logs/pipeline_*/
          retention-days: 90
      
      - name: Upload pipeline summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-summary-${{ github.run_id }}
          path: data/logs/pipeline_*/pipeline_summary.json
          retention-days: 90
      
      - name: Create failure issue
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Try to find pipeline summary
            let summary = 'Pipeline failed - check logs';
            try {
              const logDir = 'data/logs';
              const dirs = fs.readdirSync(logDir)
                .filter(d => d.startsWith('pipeline_'))
                .sort()
                .reverse();
              
              if (dirs.length > 0) {
                const summaryPath = path.join(logDir, dirs[0], 'pipeline_summary.json');
                if (fs.existsSync(summaryPath)) {
                  const data = JSON.parse(fs.readFileSync(summaryPath));
                  const lastStage = data.stages[data.stages.length - 1];
                  summary = `Pipeline failed at stage: ${lastStage?.name || 'unknown'}\n\nRun ID: ${data.run_id}\nTimestamp: ${data.timestamp}`;
                }
              }
            } catch (e) {
              // Ignore errors reading summary
            }
            
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `ðŸš¨ Pipeline Failed - ${new Date().toISOString().split('T')[0]}`,
              body: summary + `\n\n[View Run](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`,
              labels: ['pipeline-failure', 'automated']
            })
      
      - name: Fail job if pipeline failed
        if: steps.pipeline.outcome != 'success'
        run: exit 1

